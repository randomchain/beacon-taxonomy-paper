\subsection{Randomness}\label{sub:prelims_randomness}
\stefan{this subsection is not very comprehensive (nor comprehnsible): step back a bit more and think what really needs to be said here (which will also be relevant later)}

\stefan{Intuitively speaking, randomness is...} Randomness is a lack of predictability in events.
The unpredictability of a data source is measured in entropy.
\stefan{define entropy, define source: formally}
High levels of entropy makes it hard to predict the data produced by the source, and can be obtained from data with lower levels \stefan{lower levels of what?} by using extractor functions~\cite{pseudorandomness}.

\citet{bonneau2015bitcoin} define an extractor function as $y = \text{Ext}_k(x)$. \stefan{before you say what it does explain the intuition of extractor functions and why they are useful}
An extractor $\text{Ext}$ is applied on an $n$-bit input $x$ of \enquote{sufficient} entropy.
\stefan{I think before you go into details discsions of extracturs you should maybe say more about the basic stuff like how one can test randomness for example?}
The output $y$ is $m$ bits of \enquote{high} entropy, where $m < n$. The key $k$ is used to select from a family of extractors.
They further define \enquote{sufficient} to be that the min-entropy is at least $m$ bits, and \enquote{high} entropy to be that there is only an negligible difference between the output $y$ and an $m$-bit uniform distribution.
As such, it is able to convert weak random sources into a highly random output, that statistically appears to be uniformly distributed.

%This description is similar to a pseudorandom generator. Fra WIKIPEDIA: the general PRG definition does not specify that a weakly random source must be used, and while in the case of an extractor, the output should be statistically close to uniform, in a PRG it is only required to be computationally indistinguishable from uniform, a somewhat weaker concept.

\citet{dodis2004randomness} define min-entropy as the minimum integer $m$ such that for all bit strings $x$ in a probability distribution $\chi$, $\text{Pr}_\chi(x) \leq 2^{-m}$.
In other words, it is the least amount of bits required to describe the worst-case output.
Therefore it is a conservative way of measuring unpredictability.
The min-entropy of a coin toss ($p = \frac{1}{2}$) is $-\log_2(\frac{1}{2}) = 1$ bit. Similarly, throwing a die will require $-\log_2(\frac{1}{6}) \approx 2.6$ bits to describe (and since min-entropy is the minimum \emph{integer}, 3 bits are needed).
If the die is loaded, such that one outcome is more likely, the min-entropy of will be the number of bits required to describe the most likely outcome.

%Randomness can be seen as unpredictable information. Ideally, it can not be predicted or influenced in any way, hence why it can be used as a source of impartial decisions, e.g.\ for lotteries.
%However, computers are not random. Computers are intrinsically deterministic, and thus can not produce true randomness. Instead, they can generate it by observing highly unpredictable information like atmospheric noise and radioactive decay, or use complicated algorithms to generate pseudorandomness that seems unpredictable~\cite{randomsources}.
%The predictability of a source of data is measured in entropy.
